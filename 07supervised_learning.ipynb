{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning with SKLearn\n",
    "In this notebook we will explain briefly the most used supervised learning algorithms for both classificaiton and regression and we will apply them to a real datasets and compare the performances. We will use the Python package [SKLearn](https://scikit-learn.org/) that implements a lot of machine learning algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "For the regression tasks we will use the Diabetes dataset represents patients in a 10-dimensional feature space, where the features represent:\n",
    "- age age in years\n",
    "- sex\n",
    "- bmi body mass index\n",
    "- bp average blood pressure\n",
    "- s1 tc, total serum cholesterol\n",
    "- s2 ldl, low-density lipoproteins\n",
    "- s3 hdl, high-density lipoproteins\n",
    "- s4 tch, total cholesterol / HDL\n",
    "- s5 ltg, possibly log of serum triglycerides level\n",
    "- s6 glu, blood sugar level\n",
    "\n",
    "The feature variables are mean centered and scaled by the standard deviation times the square root of n_samples.\n",
    "The targets $Y$ are integers in the range $[25,346]$ and are a quantitative measure of disease progression one year after the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10),(442,)\n"
     ]
    }
   ],
   "source": [
    "X,Y = load_diabetes(return_X_y=True)\n",
    "print(f\"{X.shape},{Y.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 90% of the data for training and 10% for testing. First we shuffle the data so that we can assume that data are not ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397, 10),(397,)\n",
      "(45, 10),(45,)\n"
     ]
    }
   ],
   "source": [
    "X,Y = sklearn.utils.shuffle(X,Y,random_state=9)\n",
    "\n",
    "X_train = X[0:int(0.9 * X.shape[0])]\n",
    "Y_train = Y[0:int(0.9 * X.shape[0])]\n",
    "X_test  = X[int(0.9 * X.shape[0]):]\n",
    "Y_test  = Y[int(0.9 * X.shape[0]):]\n",
    "\n",
    "print(f\"{X_train.shape},{Y_train.shape}\")\n",
    "print(f\"{X_test.shape},{Y_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors\n",
    "Each estimators in sklearn have a score method providing a default evaluation criterion for the problem they are designed to solve. From the documentation we read that the score method of the KNeighborsClassifier returns the coefficient of determination. that is defined as $(1-\\frac{u}{v})$, where $u$ is the residual sum of squares `((y_true - y_pred)** 2).sum()` and $v$ is the total sum of squares `((y_true - y_true.mean()) ** 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \n",
    "score of 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination (Trainset): 0.17884130982367757\n",
      "Coefficient of determination (Testset): 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train,Y_train)\n",
    "print(f\"Coefficient of determination (Trainset): {model.score(X_train,Y_train)}\")\n",
    "print(f\"Coefficient of determination (Testset): {model.score(X_test,Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some predictions on the Testset:\n",
      "\t\tY_true:[138.  91.  99. 170. 317. 281. 142. 144. 155. 280.] \n",
      "\t\tY_pred:[144. 171. 220.  87. 192. 109. 107.  25. 150. 195.]\n",
      "Some predictions on the Trainset:\n",
      "\t\tY_true:[104. 118. 186. 132. 199. 215. 279. 135.  65.  70.],\n",
      "\t\tY_pred:[ 55.  65. 186.  44.  71.  67. 109.  50.  65.  71.]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(f\"Some predictions on the Testset:\\n\\t\\tY_true:{Y_test[0:10]} \\n\\t\\tY_pred:{Y_pred[0:10]}\")\n",
    "Y_pred = model.predict(X_train)\n",
    "print(f\"Some predictions on the Trainset:\\n\\t\\tY_true:{Y_train[0:10]},\\n\\t\\tY_pred:{Y_pred[0:10]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "This is the standard Linear regression we implemented from scratch in the previous lecture. The minimization objective is:\n",
    "$$\n",
    "|| \\mathbf{y} - \\mathbb{X} \\mathbf{w} ||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination (Trainset): 0.5413921609396912\n",
      "Coefficient of determination (Testset): 0.29612044394293746\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,Y_train)\n",
    "print(f\"Coefficient of determination (Trainset): {model.score(X_train,Y_train)}\")\n",
    "print(f\"Coefficient of determination (Testset): {model.score(X_test,Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some predictions on the Testset:\n",
      "\t\tY_true:[138  91  99 170 317 281 142 144 155 280] \n",
      "\t\tY_pred:[171 150 231  90 224 196 190 124 219 235]\n",
      "Some predictions on the Trainset:\n",
      "\t\tY_true:[104 118 186 132 199 215 279 135  65  70],\n",
      "\t\tY_pred:[ 75  96 202 121 111 248 216 126 122  62]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(f\"Some predictions on the Testset:\\n\\t\\tY_true:{Y_test[0:10].astype(np.int32)} \\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")\n",
    "Y_pred = model.predict(X_train)\n",
    "print(f\"Some predictions on the Trainset:\\n\\t\\tY_true:{Y_train[0:10].astype(np.int32)},\\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "As we saw in the previous lectures, the capacity of a Machine Learning model must be appropriate for the specific task. A model with an unnecessary high capacity could result in overfitting, while a model with too low capacity would not be able to fit the data (underfitting). Regularization is an indictive bias that results in favoring some hypotheses over others. The most used regularization techniques are:\n",
    "- __L1 regularization__ where the L1 norm of the parameters:\n",
    "$$\n",
    "L1(w\\in \\mathbb{R}^d) = \\sum_i^d |w_i|\n",
    "$$\n",
    "is minimized together with the objective. The linear regression with L1 regularization is called Lasso Regression.\n",
    "- __L2 regularization__ where the L2 norm of the parameters:\n",
    "$$\n",
    "L2(w\\in \\mathbb{R}^d) = \\sqrt{\\sum_i^d w_i^2}\n",
    "$$\n",
    "is minimized together with the objective. The linear regression with L2 regularization is called Ridge Regression.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination (Trainset): 0.382943118645549\n",
      "Coefficient of determination (Testset): 0.21692207839766886\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha=1.)\n",
    "model.fit(X_train,Y_train)\n",
    "print(f\"Coefficient of determination (Trainset): {model.score(X_train,Y_train)}\")\n",
    "print(f\"Coefficient of determination (Testset): {model.score(X_test,Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some predictions on the Testset:\n",
      "\t\tY_true:[138  91  99 170 317 281 142 144 155 280] \n",
      "\t\tY_pred:[158 140 189 125 184 164 168 130 167 177]\n",
      "Some predictions on the Trainset:\n",
      "\t\tY_true:[104 118 186 132 199 215 279 135  65  70],\n",
      "\t\tY_pred:[111 127 187 131 128 197 188 127 135 103]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(f\"Some predictions on the Testset:\\n\\t\\tY_true:{Y_test[0:10].astype(np.int32)} \\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")\n",
    "Y_pred = model.predict(X_train)\n",
    "print(f\"Some predictions on the Trainset:\\n\\t\\tY_true:{Y_train[0:10].astype(np.int32)},\\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination (Trainset): 0.4613537648625582\n",
      "Coefficient of determination (Testset): 0.32552492063229144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge(alpha=1)\n",
    "model.fit(X_train,Y_train)\n",
    "print(f\"Coefficient of determination (Trainset): {model.score(X_train,Y_train)}\")\n",
    "print(f\"Coefficient of determination (Testset): {model.score(X_test,Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some predictions on the Testset:\n",
      "\t\tY_true:[138  91  99 170 317 281 142 144 155 280] \n",
      "\t\tY_pred:[178 163 185 102 210 174 177 124 201 193]\n",
      "Some predictions on the Trainset:\n",
      "\t\tY_true:[104 118 186 132 199 215 279 135  65  70],\n",
      "\t\tY_pred:[100 130 193 140 131 208 193 133 143  89]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(f\"Some predictions on the Testset:\\n\\t\\tY_true:{Y_test[0:10].astype(np.int32)} \\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")\n",
    "Y_pred = model.predict(X_train)\n",
    "print(f\"Some predictions on the Trainset:\\n\\t\\tY_true:{Y_train[0:10].astype(np.int32)},\\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
