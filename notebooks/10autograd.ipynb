{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "The Deep Learning frameworks like [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) implement automatic differentiation. In few words a they allow to compute the derivatives of a composite function with respect to the inputs building a computational graph. The computational graph has nodes for each intermediate data and operation. It can be executed in forward mode, to compute the output from the input, and in backward mode, to compute the derivative of each intermediate node with respect to the output. Let's implement the computational graph for the function:\n",
    "$$\n",
    "f(x_1,x_2) = ln(x_1^2 + \\sqrt{x})\n",
    "$$\n",
    "<p align=\"center\">\n",
    "  <img src=\"../imgs/autograd.png\"/ width=50%>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the gradient is stored only for the leaf nodes, to store the gradient of non leaf nodes one must call the method `tensor.retain_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.tensor([2.],dtype=torch.float32, requires_grad=True)\n",
    "x2 = torch.tensor([3.],dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = torch.pow(x1,2)\n",
    "h2 = torch.sqrt(x2)\n",
    "h1.retain_grad()\n",
    "h2.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = h1 + h2\n",
    "z.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.log(z)\n",
    "o.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do/do = tensor([1.])\n",
      "do/dz = tensor([0.1745])\n",
      "do/dh1 = tensor([0.1745])\n",
      "do/dh2 = tensor([0.1745])\n",
      "do/dx1 = tensor([0.6978])\n",
      "do/dx2 = tensor([0.0504])\n"
     ]
    }
   ],
   "source": [
    "print(f\"do/do = {o.grad}\")\n",
    "print(f\"do/dz = {z.grad}\")\n",
    "print(f\"do/dh1 = {h1.grad}\")\n",
    "print(f\"do/dh2 = {h2.grad}\")\n",
    "print(f\"do/dx1 = {x1.grad}\")\n",
    "print(f\"do/dx2 = {x2.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
