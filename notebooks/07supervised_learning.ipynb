{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "In this notebook we will explain briefly the most used supervised learning algorithms for both classificaiton and regression and we will apply them to a real datasets and compare the performances. We will use the Python package [SKLearn](https://scikit-learn.org/) that implements a lot of machine learning algorithms.\n",
    "\n",
    "Before comparing the models on the selected dataset let us introduce a fundamental concept that will help us understand better the concepts of overfitting, underfitting and the regularization techniques: the bias-variance decomposition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "The curs of dimensionality refers to the problems encountered when learning in high dimensions. In particular the volume of the data space increases exponentially with respect to the number of dimensions, and the space becomes sparse. This means that the density of the data space decreases when the number of dimensions increases, meaking more difficult to learn patterns in data because I've fewer examples per volume unit. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Variance Decomposition\n",
    "The error of an estimator (that in our case is the Machine Learning model) can be decomposed into two terms: a bias term and a variance term. Imagine we could train the same model on many different datasets. The bias tells how much is the error if we take the average of the predictions from these models, while the variance term is the variance of predictions obtained by these models. Matematically \n",
    "$$\n",
    "Err(x) = \\mathbb{E}_{\\hat{f}}[(\\mathbb{E}_{\\hat{f}}[\\hat{f}(x)]-f(x))^2] + \\mathbb{E}_{\\hat{f}}[(\\hat{f}(x)-\\mathbb{E}_{\\hat{f}}[\\hat{f}(x)])^2]\n",
    "$$\n",
    "Where $\\hat{f}$ is the predicted function and $f$ is the true function. Note the expectation over the models trained on different datasets.\n",
    "Bias and variance are represented in the following image, borrowed from this [great article](http://scott.fortmann-roe.com/docs/BiasVariance.html) on bias-variance decomposition:\n",
    "<p align=\"center\">\n",
    "  <img src=\"../imgs/biasvariance.png\"/ width=30%>\n",
    "</p>\n",
    "As we can see from the picture, an estimator with high bias and low variance will return on average a value that is different from the true value, but the predicted values will be similar. While an estimator with low bias and high variance will return the correct value on average, but the predicted values will change a lot. \n",
    "The bias variance decomposition is a different way to express the concepts of overfitting and underfitting. If we increase the model capacity we reduce the bias but we might increase variance and vice versa. As shown by the figure:\n",
    "<p align=\"center\">\n",
    "  <img src=\"../imgs/overunderfitting.png\"/ width=30%>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "For the regression tasks we will use the Diabetes dataset represents patients in a 10-dimensional feature space, where the features represent:\n",
    "- age age in years\n",
    "- sex\n",
    "- bmi body mass index\n",
    "- bp average blood pressure\n",
    "- s1 tc, total serum cholesterol\n",
    "- s2 ldl, low-density lipoproteins\n",
    "- s3 hdl, high-density lipoproteins\n",
    "- s4 tch, total cholesterol / HDL\n",
    "- s5 ltg, possibly log of serum triglycerides level\n",
    "- s6 glu, blood sugar level\n",
    "\n",
    "The feature variables are mean centered and scaled by the standard deviation times the square root of n_samples.\n",
    "The targets $Y$ are integers in the range $[25,346]$ and are a quantitative measure of disease progression one year after the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10),(442,)\n"
     ]
    }
   ],
   "source": [
    "X,Y = load_diabetes(return_X_y=True)\n",
    "print(f\"{X.shape},{Y.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 90% of the data for training and 10% for testing. First we shuffle the data so that we can assume that data are not ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397, 10),(397,)\n",
      "(45, 10),(45,)\n"
     ]
    }
   ],
   "source": [
    "X,Y = sklearn.utils.shuffle(X,Y,random_state=9)\n",
    "\n",
    "X_train = X[0:int(0.9 * X.shape[0])]\n",
    "Y_train = Y[0:int(0.9 * X.shape[0])]\n",
    "X_test  = X[int(0.9 * X.shape[0]):]\n",
    "Y_test  = Y[int(0.9 * X.shape[0]):]\n",
    "\n",
    "print(f\"{X_train.shape},{Y_train.shape}\")\n",
    "print(f\"{X_test.shape},{Y_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors\n",
    "Each estimators in sklearn have a score method providing a default evaluation criterion for the problem they are designed to solve. From the documentation we read that the score method of the KNeighborsClassifier returns the coefficient of determination. that is defined as $(1-\\frac{u}{v})$, where $u$ is the residual sum of squares `((y_true - y_pred)** 2).sum()` and $v$ is the total sum of squares `((y_true - y_true.mean()) ** 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \n",
    "score of 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination (Trainset): 0.17884130982367757\n",
      "Coefficient of determination (Testset): 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train,Y_train)\n",
    "print(f\"Coefficient of determination (Trainset): {model.score(X_train,Y_train)}\")\n",
    "print(f\"Coefficient of determination (Testset): {model.score(X_test,Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some predictions on the Testset:\n",
      "\t\tY_true:[138.  91.  99. 170. 317. 281. 142. 144. 155. 280.] \n",
      "\t\tY_pred:[144. 171. 220.  87. 192. 109. 107.  25. 150. 195.]\n",
      "Some predictions on the Trainset:\n",
      "\t\tY_true:[104. 118. 186. 132. 199. 215. 279. 135.  65.  70.],\n",
      "\t\tY_pred:[ 55.  65. 186.  44.  71.  67. 109.  50.  65.  71.]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(f\"Some predictions on the Testset:\\n\\t\\tY_true:{Y_test[0:10]} \\n\\t\\tY_pred:{Y_pred[0:10]}\")\n",
    "Y_pred = model.predict(X_train)\n",
    "print(f\"Some predictions on the Trainset:\\n\\t\\tY_true:{Y_train[0:10]},\\n\\t\\tY_pred:{Y_pred[0:10]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "This is the standard Linear regression we implemented from scratch in the previous lecture. The minimization objective is:\n",
    "$$\n",
    "|| \\mathbf{y} - \\mathbb{X} \\mathbf{w} ||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination (Trainset): 0.5413921609396912\n",
      "Coefficient of determination (Testset): 0.29612044394293746\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,Y_train)\n",
    "print(f\"Coefficient of determination (Trainset): {model.score(X_train,Y_train)}\")\n",
    "print(f\"Coefficient of determination (Testset): {model.score(X_test,Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some predictions on the Testset:\n",
      "\t\tY_true:[138  91  99 170 317 281 142 144 155 280] \n",
      "\t\tY_pred:[171 150 231  90 224 196 190 124 219 235]\n",
      "Some predictions on the Trainset:\n",
      "\t\tY_true:[104 118 186 132 199 215 279 135  65  70],\n",
      "\t\tY_pred:[ 75  96 202 121 111 248 216 126 122  62]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(f\"Some predictions on the Testset:\\n\\t\\tY_true:{Y_test[0:10].astype(np.int32)} \\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")\n",
    "Y_pred = model.predict(X_train)\n",
    "print(f\"Some predictions on the Trainset:\\n\\t\\tY_true:{Y_train[0:10].astype(np.int32)},\\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "Regularization is an indictive bias that results in favoring some hypotheses over others. The most used regularization techniques are:\n",
    "- __L1 regularization__ where the L1 norm of the parameters:\n",
    "$$\n",
    "L1(w\\in \\mathbb{R}^d) = \\sum_i^d |w_i|\n",
    "$$\n",
    "is minimized together with the objective. The linear regression with L1 regularization is called Lasso Regression. This technique promote a sparse solution (in which most weights are zero). This helps to escape the curse of dimensionality because only relevant features (dimensions) contribute to the solution.\n",
    "\n",
    "- __L2 regularization__ where the L2 norm of the parameters:\n",
    "$$\n",
    "L2(w\\in \\mathbb{R}^d) = \\sqrt{\\sum_i^d w_i^2}\n",
    "$$\n",
    "is minimized together with the objective. The linear regression with L2 regularization is called Ridge Regression. This regularization technique reduces the variance at the expense of the bias.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination (Trainset): 0.382943118645549\n",
      "Coefficient of determination (Testset): 0.21692207839766886\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha=1.)\n",
    "model.fit(X_train,Y_train)\n",
    "print(f\"Coefficient of determination (Trainset): {model.score(X_train,Y_train)}\")\n",
    "print(f\"Coefficient of determination (Testset): {model.score(X_test,Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some predictions on the Testset:\n",
      "\t\tY_true:[138  91  99 170 317 281 142 144 155 280] \n",
      "\t\tY_pred:[158 140 189 125 184 164 168 130 167 177]\n",
      "Some predictions on the Trainset:\n",
      "\t\tY_true:[104 118 186 132 199 215 279 135  65  70],\n",
      "\t\tY_pred:[111 127 187 131 128 197 188 127 135 103]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(f\"Some predictions on the Testset:\\n\\t\\tY_true:{Y_test[0:10].astype(np.int32)} \\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")\n",
    "Y_pred = model.predict(X_train)\n",
    "print(f\"Some predictions on the Trainset:\\n\\t\\tY_true:{Y_train[0:10].astype(np.int32)},\\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination (Trainset): 0.4613537648625582\n",
      "Coefficient of determination (Testset): 0.32552492063229144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge(alpha=1)\n",
    "model.fit(X_train,Y_train)\n",
    "print(f\"Coefficient of determination (Trainset): {model.score(X_train,Y_train)}\")\n",
    "print(f\"Coefficient of determination (Testset): {model.score(X_test,Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some predictions on the Testset:\n",
      "\t\tY_true:[138  91  99 170 317 281 142 144 155 280] \n",
      "\t\tY_pred:[178 163 185 102 210 174 177 124 201 193]\n",
      "Some predictions on the Trainset:\n",
      "\t\tY_true:[104 118 186 132 199 215 279 135  65  70],\n",
      "\t\tY_pred:[100 130 193 140 131 208 193 133 143  89]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(f\"Some predictions on the Testset:\\n\\t\\tY_true:{Y_test[0:10].astype(np.int32)} \\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")\n",
    "Y_pred = model.predict(X_train)\n",
    "print(f\"Some predictions on the Trainset:\\n\\t\\tY_true:{Y_train[0:10].astype(np.int32)},\\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Trick\n",
    "In linear regression models the prediciton can be expressed in terms of a similarity measure $K(x_i,x_j)$, where $x_i$ and $x_j$ are vectors in the input space.\n",
    "Given the input data matrix:\n",
    "$$\n",
    "\\mathbb{X} = \n",
    "\\begin{pmatrix}\n",
    "1 & x_1^1 & ... & x_1^d \\\\\n",
    ".&.&.&...&.\\\\\n",
    ".&.&.&...&.\\\\\n",
    ".&.&.&...&.\\\\\n",
    "1 x_n^1 & ... & x_n^d \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "(where d is the dimensionality of the data space and n is the number of samples in the traininset) and the vectors with the true values:\n",
    "$$\n",
    "\\mathbb{Y} = \\begin{pmatrix}y_1 & . & . & . &y_n\\end{pmatrix}\n",
    "$$\n",
    "we know that the closed form solution for the parameters is:\n",
    "$$\n",
    "\\mathbf{w} = \\frac{\\mathbb{X}^T\\mathbb{Y}}{(\\mathbb{X}^T\\mathbb{X})-1}\n",
    "$$\n",
    "Now given a input matrix $\\hat{\\mathbb{X}}$ organized as $\\mathbb{X}$ we can compute the vector with the predictions as:\n",
    "$$\n",
    "\\hat{\\mathbb{Y}} = \\hat{\\mathbb{X}}\\mathbf{w} = \\frac{\\hat{\\mathbb{X}}\\mathbb{X}^T\\mathbb{Y}}{(\\mathbb{X}^T\\mathbb{X})^{-1}}\n",
    "$$\n",
    "Now we can observe that the elements of the matrices $\\hat{\\mathbb{X}}\\mathbb{X}^T$ and $\\mathbb{X}^T\\mathbb{X}$ can be all expressed as inner products:\n",
    "$$\n",
    "(\\hat{\\mathbb{X}}\\mathbb{X}^T)_{ij} = <\\hat{\\mathbf{x}}_i,\\mathbf{x}_j>\n",
    "$$\n",
    "$$\n",
    "(\\mathbb{X}\\mathbb{X}^T)_{ij} = <\\mathbf{x}_i,\\mathbf{x}_j>\n",
    "$$\n",
    "If we want express the points in a different feature space, we can apply the (also nonlinear) function $\\phi$ to each point:\n",
    "$$\n",
    "\\Phi(x) = \n",
    "\\begin{pmatrix}\\phi_1(x)&.&.&\\phi_n(x)\\end{pmatrix}\n",
    "$$\n",
    "so that instead of the dot products between input vectors $<\\mathbf{x_1},\\mathbf{x_j}>$ we need to compute the dot products between feature vectors\n",
    "$$\n",
    "<\\Phi(x_i),\\Phi(x_j)>\n",
    "$$\n",
    "and feature vectors can have an arbitrary number of dimensions, even infinite.\n",
    "The kernel trick consists in replacing $<\\Phi(x_i),\\Phi(x_j)>$ with an arbitrary similarity measure $K(x_i,x_j)$ that corresponds to the inner product in a feature space. In order to correspond to a valid inner product K must be positive-definite. Some common choices are:\n",
    "- __the polynomial kernel__ $k(x,z) = (1+x^Tz)^d$ that correspond to the inner product in a feature space in which $\\Phi(x)$ is the vector formed by all the products between the components of $x \\in R^n$ up to degree d.\n",
    "- __the gaussian kernel__  $k(x,z) = e^{-\\gamma ||x-z||^2}$ that corresponds to an infinite dimensional feature space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression with Kernel Trick\n",
    "With gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination (Trainset): 0.5016447488558985\n",
      "Coefficient of determination (Testset): 0.3278467287314403\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import math\n",
    "\n",
    "def gaussian_kernel(x,y):\n",
    "    return math.exp((-np.sum((x-y)**2)))\n",
    "\n",
    "model = KernelRidge(alpha=1,kernel=gaussian_kernel)\n",
    "model.fit(X_train,Y_train)\n",
    "print(f\"Coefficient of determination (Trainset): {model.score(X_train,Y_train)}\")\n",
    "print(f\"Coefficient of determination (Testset): {model.score(X_test,Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some predictions on the Testset:\n",
      "\t\tY_true:[138  91  99 170 317 281 142 144 155 280] \n",
      "\t\tY_pred:[176 158 199  91 211 182 182 124 208 206]\n",
      "Some predictions on the Trainset:\n",
      "\t\tY_true:[104 118 186 132 199 215 279 135  65  70],\n",
      "\t\tY_pred:[ 90 122 196 136 124 222 201 131 139  78]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(f\"Some predictions on the Testset:\\n\\t\\tY_true:{Y_test[0:10].astype(np.int32)} \\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")\n",
    "Y_pred = model.predict(X_train)\n",
    "print(f\"Some predictions on the Trainset:\\n\\t\\tY_true:{Y_train[0:10].astype(np.int32)},\\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination (Trainset): 0.5371856613273562\n",
      "Coefficient of determination (Testset): 0.3287911277020671\n"
     ]
    }
   ],
   "source": [
    "def poly_kernel(x,y,deg=5):\n",
    "    return (1 + x.T @ y)**deg\n",
    "\n",
    "model = KernelRidge(alpha=1,kernel=poly_kernel)\n",
    "model.fit(X_train,Y_train)\n",
    "print(f\"Coefficient of determination (Trainset): {model.score(X_train,Y_train)}\")\n",
    "print(f\"Coefficient of determination (Testset): {model.score(X_test,Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some predictions on the Testset:\n",
      "\t\tY_true:[138  91  99 170 317 281 142 144 155 280] \n",
      "\t\tY_pred:[173 151 211  88 219 185 182 123 215 218]\n",
      "Some predictions on the Trainset:\n",
      "\t\tY_true:[104 118 186 132 199 215 279 135  65  70],\n",
      "\t\tY_pred:[ 82 109 200 128 118 234 207 127 129  73]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(f\"Some predictions on the Testset:\\n\\t\\tY_true:{Y_test[0:10].astype(np.int32)} \\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")\n",
    "Y_pred = model.predict(X_train)\n",
    "print(f\"Some predictions on the Trainset:\\n\\t\\tY_true:{Y_train[0:10].astype(np.int32)},\\n\\t\\tY_pred:{Y_pred[0:10].astype(np.int32)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
